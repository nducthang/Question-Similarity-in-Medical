{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {}\n",
    "param['learning_rate'] = 0.14\n",
    "param['boosting)type'] = 'dart'\n",
    "param['objective'] = 'binary'\n",
    "param['metric'] = 'binary_logloss'\n",
    "param['sub_feature'] = 0.5\n",
    "param['num_leaves'] = 512\n",
    "param['min_data'] = 50\n",
    "param['min_hessian'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/kaggle/train.csv')\n",
    "df_test = pd.read_csv('./data/kaggle/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original data: X_train: (404290, 6), X_test: (2345796, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original data: X_train: {}, X_test: {}\".format(df_train.shape, df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>What is the step by step guide to invest in sh...</td>\n      <td>What is the step by step guide to invest in sh...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n      <td>What would happen if the Indian government sto...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>5</td>\n      <td>6</td>\n      <td>How can I increase the speed of my internet co...</td>\n      <td>How can Internet speed be increased by hacking...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>7</td>\n      <td>8</td>\n      <td>Why am I mentally very lonely? How can I solve...</td>\n      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>9</td>\n      <td>10</td>\n      <td>Which one dissolve in water quikly sugar, salt...</td>\n      <td>Which fish would survive in salt water?</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   test_id                                          question1  \\\n",
       "0        0  How does the Surface Pro himself 4 compare wit...   \n",
       "1        1  Should I have a hair transplant at age 24? How...   \n",
       "2        2  What but is the best way to send money from Ch...   \n",
       "3        3                        Which food not emulsifiers?   \n",
       "4        4                   How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                           question2  \n",
       "0  Why did Microsoft choose core m3 and not core ...  \n",
       "1        How much cost does hair transplant require?  \n",
       "2                      What you send money to China?  \n",
       "3                                  What foods fibre?  \n",
       "4                     How their can I start reading?  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>test_id</th>\n      <th>question1</th>\n      <th>question2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>How does the Surface Pro himself 4 compare wit...</td>\n      <td>Why did Microsoft choose core m3 and not core ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Should I have a hair transplant at age 24? How...</td>\n      <td>How much cost does hair transplant require?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>What but is the best way to send money from Ch...</td>\n      <td>What you send money to China?</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Which food not emulsifiers?</td>\n      <td>What foods fibre?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>How \"aberystwyth\" start reading?</td>\n      <td>How their can I start reading?</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = (\" \".join(train_qs)).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "counts = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(count, eps = 10000, min_count=2):\n",
    "    return 0 if count < min_count else 1/(count+eps)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    id  qid1  qid2                                          question1  \\\n",
       "0  0.0   1.0   2.0  What is the step by step guide to invest in sh...   \n",
       "1  1.0   3.0   4.0  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  2.0   5.0   6.0  How can I increase the speed of my internet co...   \n",
       "3  3.0   7.0   8.0  Why am I mentally very lonely? How can I solve...   \n",
       "4  4.0   9.0  10.0  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  test_id  \n",
       "0  What is the step by step guide to invest in sh...           0.0      NaN  \n",
       "1  What would happen if the Indian government sto...           0.0      NaN  \n",
       "2  How can Internet speed be increased by hacking...           0.0      NaN  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...           0.0      NaN  \n",
       "4            Which fish would survive in salt water?           0.0      NaN  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n      <th>test_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>What is the step by step guide to invest in sh...</td>\n      <td>What is the step by step guide to invest in sh...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n      <td>What would happen if the Indian government sto...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>How can I increase the speed of my internet co...</td>\n      <td>How can Internet speed be increased by hacking...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.0</td>\n      <td>7.0</td>\n      <td>8.0</td>\n      <td>Why am I mentally very lonely? How can I solve...</td>\n      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n      <td>9.0</td>\n      <td>10.0</td>\n      <td>Which one dissolve in water quikly sugar, salt...</td>\n      <td>Which fish would survive in salt water?</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "source": [
    "# Tạo các đặc trưng "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_shares(row):\n",
    "    # Lấy danh sách các từ trong q1 khác stop word\n",
    "    q1_list = str(row['question1']).lower().split()\n",
    "    q1 = set(q1_list)\n",
    "    q1words = q1.difference(stops)\n",
    "    if len(q1words) == 0:\n",
    "        return '0:0:0:0:0:0:0:0'\n",
    "    \n",
    "    # Lấy danh sách các từ trong q2 khác stop word\n",
    "    q2_list = str(row['question2']).lower().split()\n",
    "    q2 = set(q2_list)\n",
    "    q2words = q2.difference(stops)\n",
    "    if len(q2words) == 0:\n",
    "        return '0:0:0:0:0:0:0:0'\n",
    "\n",
    "    words_hamming = sum(1 for i in zip(q1_list, q2_list) if i[0]==i[1])/max(len(q1_list), len(q2_list))\n",
    "\n",
    "    # Trả về các từ chung giữa qi và stops\n",
    "    q1stops = q1.intersection(stops)\n",
    "    q2stops = q2.intersection(stops)\n",
    "\n",
    "    # Trả về các cặp 2gram của câu hỏi\n",
    "    q1_2gram = set([i for i in zip(q1_list, q1_list[1:])])\n",
    "    q2_2gram = set([i for i in zip(q2_list, q2_list[1:])])\n",
    "\n",
    "    # Các cặp 2gram chung giữa 2 cây hỏi\n",
    "    shared_2gram = q1_2gram.intersection(q2_2gram)\n",
    "\n",
    "    # Các từ chung giữa 2 câu hỏi (Đã loại bỏ stop word)\n",
    "    shared_words = q1words.intersection(q2words)\n",
    "\n",
    "    # weight là dict chứa từ và trọng số của từ\n",
    "    # Lấy vector chứa các trọng số của từ chung giữa 2 câu\n",
    "    shared_weights = [weights.get(w, 0) for w in shared_words]\n",
    "    # Lấy vector chứa các trọng số của từng câu\n",
    "    q1_weights = [weights.get(w, 0) for w in q1words]\n",
    "    q2_weights = [weights.get(w, 0) for w in q2words]\n",
    "    # Nối 2 list lại\n",
    "    total_weights = q1_weights + q2_weights\n",
    "\n",
    "    # Tính toán các đặc trưng\n",
    "    R1 = np.sum(shared_weights)/np.sum(total_weights) # TF-IDF share\n",
    "    R2 = len(shared_words)/(len(q1words)+len(q2words)-len(shared_words)) # count share\n",
    "    R31 = len(q1stops) / len(q1words) #stops in q1\n",
    "    R32 = len(q2stops) / len(q2words) #stops in q2\n",
    "\n",
    "    Rcosine_denominator = (np.sqrt(np.dot(q1_weights,q1_weights))*np.sqrt(np.dot(q2_weights,q2_weights)))\n",
    "    Rcosine = np.dot(shared_weights, shared_weights)/Rcosine_denominator # Khoảng cách consine\n",
    "\n",
    "    if len(q1_2gram) + len(q2_2gram) == 0:\n",
    "        R2gram = 0\n",
    "    else:\n",
    "        R2gram = len(shared_2gram) / (len(q1_2gram) + len(q2_2gram))\n",
    "    return '{}:{}:{}:{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32, R2gram, Rcosine, words_hamming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/thang/env/lib/python3.6/site-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/thang/env/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/thang/env/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "df['word_shares'] = df.apply(word_shares, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    id  qid1  qid2                                          question1  \\\n",
       "0  0.0   1.0   2.0  What is the step by step guide to invest in sh...   \n",
       "1  1.0   3.0   4.0  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  2.0   5.0   6.0  How can I increase the speed of my internet co...   \n",
       "3  3.0   7.0   8.0  Why am I mentally very lonely? How can I solve...   \n",
       "4  4.0   9.0  10.0  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  test_id  \\\n",
       "0  What is the step by step guide to invest in sh...           0.0      NaN   \n",
       "1  What would happen if the Indian government sto...           0.0      NaN   \n",
       "2  How can Internet speed be increased by hacking...           0.0      NaN   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...           0.0      NaN   \n",
       "4            Which fish would survive in salt water?           0.0      NaN   \n",
       "\n",
       "                                         word_shares  \n",
       "0  0.386082181176695:0.5714285714285714:4:1.0:1.2...  \n",
       "1  0.1808787650162546:0.18181818181818182:2:1.0:0...  \n",
       "2  0.17759555605674818:0.2222222222222222:2:1.333...  \n",
       "3                      0.0:0.0:0:1.5:0.8:0.0:0.0:0.0  \n",
       "4      0.0:0.0:0:0.3:0.4:0.0:0.0:0.07692307692307693  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n      <th>test_id</th>\n      <th>word_shares</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>What is the step by step guide to invest in sh...</td>\n      <td>What is the step by step guide to invest in sh...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.386082181176695:0.5714285714285714:4:1.0:1.2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n      <td>What would happen if the Indian government sto...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.1808787650162546:0.18181818181818182:2:1.0:0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>How can I increase the speed of my internet co...</td>\n      <td>How can Internet speed be increased by hacking...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.17759555605674818:0.2222222222222222:2:1.333...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.0</td>\n      <td>7.0</td>\n      <td>8.0</td>\n      <td>Why am I mentally very lonely? How can I solve...</td>\n      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0:0.0:0:1.5:0.8:0.0:0.0:0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n      <td>9.0</td>\n      <td>10.0</td>\n      <td>Which one dissolve in water quikly sugar, salt...</td>\n      <td>Which fish would survive in salt water?</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0:0.0:0:0.3:0.4:0.0:0.0:0.07692307692307693</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "source": [
    "# Chuẩn bị đặc trưng"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame()\n",
    "\n",
    "x['word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\n",
    "x['word_match_2root'] = np.sqrt(x['word_match'])\n",
    "x['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\n",
    "x['shared_count'] = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n",
    "\n",
    "x['stops1_ratio'] = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\n",
    "x['stops2_ratio'] = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\n",
    "x['shared_2gram'] = df['word_shares'].apply(lambda x: float(x.split(':')[5]))\n",
    "x['cosine'] = df['word_shares'].apply(lambda x: float(x.split(':')[6]))\n",
    "x['words_hamming'] = df['word_shares'].apply(lambda x: float(x.split(':')[7]))\n",
    "\n",
    "x['diff_stops_r'] = x['stops1_ratio'] - x['stops2_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   word_match  word_match_2root  tfidf_word_match  shared_count  stops1_ratio  \\\n",
       "0    0.386082          0.621355          0.571429           4.0      1.000000   \n",
       "1    0.180879          0.425298          0.181818           2.0      1.000000   \n",
       "2    0.177596          0.421421          0.222222           2.0      1.333333   \n",
       "3    0.000000          0.000000          0.000000           0.0      1.500000   \n",
       "4    0.000000          0.000000          0.000000           0.0      0.300000   \n",
       "\n",
       "   stops2_ratio  shared_2gram    cosine  words_hamming  diff_stops_r  \n",
       "0      1.200000      0.416667  0.795192       0.785714     -0.200000  \n",
       "1      0.333333      0.052632  0.410927       0.076923      0.666667  \n",
       "2      1.000000      0.045455  0.340883       0.142857      0.333333  \n",
       "3      0.800000      0.000000  0.000000       0.000000      0.700000  \n",
       "4      0.400000      0.000000  0.000000       0.076923     -0.100000  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word_match</th>\n      <th>word_match_2root</th>\n      <th>tfidf_word_match</th>\n      <th>shared_count</th>\n      <th>stops1_ratio</th>\n      <th>stops2_ratio</th>\n      <th>shared_2gram</th>\n      <th>cosine</th>\n      <th>words_hamming</th>\n      <th>diff_stops_r</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.386082</td>\n      <td>0.621355</td>\n      <td>0.571429</td>\n      <td>4.0</td>\n      <td>1.000000</td>\n      <td>1.200000</td>\n      <td>0.416667</td>\n      <td>0.795192</td>\n      <td>0.785714</td>\n      <td>-0.200000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.180879</td>\n      <td>0.425298</td>\n      <td>0.181818</td>\n      <td>2.0</td>\n      <td>1.000000</td>\n      <td>0.333333</td>\n      <td>0.052632</td>\n      <td>0.410927</td>\n      <td>0.076923</td>\n      <td>0.666667</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.177596</td>\n      <td>0.421421</td>\n      <td>0.222222</td>\n      <td>2.0</td>\n      <td>1.333333</td>\n      <td>1.000000</td>\n      <td>0.045455</td>\n      <td>0.340883</td>\n      <td>0.142857</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.500000</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.700000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.300000</td>\n      <td>0.400000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.076923</td>\n      <td>-0.100000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\n",
    "x['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\n",
    "x['diff_len'] = x['len_q1'] - x['len_q2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['caps_count_q1'] = df['question1'].apply(lambda x: sum(1 for i in str(x) if i.isupper()))\n",
    "x['caps_count_q2'] = df['question2'].apply(lambda x: sum(1 for i in str(x) if i.isupper()))\n",
    "x['diff_caps'] = x['caps_count_q1'] - x['caps_count_q2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đếm xem mỗi câu có bao nhiêu ký tự khác trắng\n",
    "x['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ','')))\n",
    "x['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ','')))\n",
    "x['diff_len_char'] = x['len_char_q1'] - x['len_char_q2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đếm số từ ở mỗi câu\n",
    "x['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "x['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "x['diff_len_word'] = x['len_word_q1'] - x['len_word_q2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tỉ lệ ký tự trên từ\n",
    "x['avg_world_len1'] = x['len_char_q1']/x['len_word_q1']\n",
    "x['avg_world_len2'] = x['len_char_q2']/x['len_word_q2']\n",
    "x['diff_avg_word'] = x['avg_world_len1'] = x['avg_world_len2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['exactly_same'] = (df['question1'] == df['question2']).astype(int) # giống nhau hoàn toàn hay không\n",
    "x['duplicated'] = df.duplicated(['question1', 'question2']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   word_match  word_match_2root  tfidf_word_match  shared_count  stops1_ratio  \\\n",
       "0    0.386082          0.621355          0.571429           4.0      1.000000   \n",
       "1    0.180879          0.425298          0.181818           2.0      1.000000   \n",
       "2    0.177596          0.421421          0.222222           2.0      1.333333   \n",
       "3    0.000000          0.000000          0.000000           0.0      1.500000   \n",
       "4    0.000000          0.000000          0.000000           0.0      0.300000   \n",
       "\n",
       "   stops2_ratio  shared_2gram    cosine  words_hamming  diff_stops_r  ...  \\\n",
       "0      1.200000      0.416667  0.795192       0.785714     -0.200000  ...   \n",
       "1      0.333333      0.052632  0.410927       0.076923      0.666667  ...   \n",
       "2      1.000000      0.045455  0.340883       0.142857      0.333333  ...   \n",
       "3      0.800000      0.000000  0.000000       0.000000      0.700000  ...   \n",
       "4      0.400000      0.000000  0.000000       0.076923     -0.100000  ...   \n",
       "\n",
       "   len_char_q2  diff_len_char  len_word_q1  len_word_q2  diff_len_word  \\\n",
       "0           46              7           14           12              2   \n",
       "1           76            -32            8           13             -5   \n",
       "2           50             10           14           10              4   \n",
       "3           57            -17           11            9              2   \n",
       "4           33             31           13            7              6   \n",
       "\n",
       "   avg_world_len1  avg_world_len2  diff_avg_word  exactly_same  duplicated  \n",
       "0        3.833333        3.833333       3.833333             0           0  \n",
       "1        5.846154        5.846154       5.846154             0           0  \n",
       "2        5.000000        5.000000       5.000000             0           0  \n",
       "3        6.333333        6.333333       6.333333             0           0  \n",
       "4        4.714286        4.714286       4.714286             0           0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word_match</th>\n      <th>word_match_2root</th>\n      <th>tfidf_word_match</th>\n      <th>shared_count</th>\n      <th>stops1_ratio</th>\n      <th>stops2_ratio</th>\n      <th>shared_2gram</th>\n      <th>cosine</th>\n      <th>words_hamming</th>\n      <th>diff_stops_r</th>\n      <th>...</th>\n      <th>len_char_q2</th>\n      <th>diff_len_char</th>\n      <th>len_word_q1</th>\n      <th>len_word_q2</th>\n      <th>diff_len_word</th>\n      <th>avg_world_len1</th>\n      <th>avg_world_len2</th>\n      <th>diff_avg_word</th>\n      <th>exactly_same</th>\n      <th>duplicated</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.386082</td>\n      <td>0.621355</td>\n      <td>0.571429</td>\n      <td>4.0</td>\n      <td>1.000000</td>\n      <td>1.200000</td>\n      <td>0.416667</td>\n      <td>0.795192</td>\n      <td>0.785714</td>\n      <td>-0.200000</td>\n      <td>...</td>\n      <td>46</td>\n      <td>7</td>\n      <td>14</td>\n      <td>12</td>\n      <td>2</td>\n      <td>3.833333</td>\n      <td>3.833333</td>\n      <td>3.833333</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.180879</td>\n      <td>0.425298</td>\n      <td>0.181818</td>\n      <td>2.0</td>\n      <td>1.000000</td>\n      <td>0.333333</td>\n      <td>0.052632</td>\n      <td>0.410927</td>\n      <td>0.076923</td>\n      <td>0.666667</td>\n      <td>...</td>\n      <td>76</td>\n      <td>-32</td>\n      <td>8</td>\n      <td>13</td>\n      <td>-5</td>\n      <td>5.846154</td>\n      <td>5.846154</td>\n      <td>5.846154</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.177596</td>\n      <td>0.421421</td>\n      <td>0.222222</td>\n      <td>2.0</td>\n      <td>1.333333</td>\n      <td>1.000000</td>\n      <td>0.045455</td>\n      <td>0.340883</td>\n      <td>0.142857</td>\n      <td>0.333333</td>\n      <td>...</td>\n      <td>50</td>\n      <td>10</td>\n      <td>14</td>\n      <td>10</td>\n      <td>4</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.500000</td>\n      <td>0.800000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.700000</td>\n      <td>...</td>\n      <td>57</td>\n      <td>-17</td>\n      <td>11</td>\n      <td>9</td>\n      <td>2</td>\n      <td>6.333333</td>\n      <td>6.333333</td>\n      <td>6.333333</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.300000</td>\n      <td>0.400000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.076923</td>\n      <td>-0.100000</td>\n      <td>...</td>\n      <td>33</td>\n      <td>31</td>\n      <td>13</td>\n      <td>7</td>\n      <td>6</td>\n      <td>4.714286</td>\n      <td>4.714286</td>\n      <td>4.714286</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 27 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đếm số lần xuất hiện của từ word trong câu\n",
    "def add_word_count(x, df, word):\n",
    "    x['q1_' + word] = df['question1'].apply(lambda x: (word in str(x).lower())*1)\n",
    "    x['q2_' + word] = df['question2'].apply(lambda x: (word in str(x).lower())*1)\n",
    "    x[word + '_both'] = x['q1_' + word] + x['q2_' + word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_word_count(x, df,'how')\n",
    "add_word_count(x, df,'what')\n",
    "add_word_count(x, df,'which')\n",
    "add_word_count(x, df,'who')\n",
    "add_word_count(x, df,'where')\n",
    "add_word_count(x, df,'when')\n",
    "add_word_count(x, df,'why')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['word_match', 'word_match_2root', 'tfidf_word_match', 'shared_count',\n",
       "       'stops1_ratio', 'stops2_ratio', 'shared_2gram', 'cosine',\n",
       "       'words_hamming', 'diff_stops_r', 'len_q1', 'len_q2', 'diff_len',\n",
       "       'caps_count_q1', 'caps_count_q2', 'diff_caps', 'len_char_q1',\n",
       "       'len_char_q2', 'diff_len_char', 'len_word_q1', 'len_word_q2',\n",
       "       'diff_len_word', 'avg_world_len1', 'avg_world_len2', 'diff_avg_word',\n",
       "       'exactly_same', 'duplicated', 'q1_how', 'q2_how', 'how_both', 'q1_what',\n",
       "       'q2_what', 'what_both', 'q1_which', 'q2_which', 'which_both', 'q1_who',\n",
       "       'q2_who', 'who_both', 'q1_where', 'q2_where', 'where_both', 'q1_when',\n",
       "       'q2_when', 'when_both', 'q1_why', 'q2_why', 'why_both'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         word_match  word_match_2root  tfidf_word_match  shared_count  \\\n",
       "count  2.749809e+06      2.749809e+06      2.750086e+06  2.750086e+06   \n",
       "mean   1.473579e-01      3.164050e-01      2.019946e-01  1.600848e+00   \n",
       "std    1.282001e-01      2.173611e-01      2.017327e-01  1.543970e+00   \n",
       "min    0.000000e+00      0.000000e+00      0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00      0.000000e+00      0.000000e+00  0.000000e+00   \n",
       "50%    1.340544e-01      3.661345e-01      1.666667e-01  1.000000e+00   \n",
       "75%    2.319422e-01      4.816038e-01      3.000000e-01  2.000000e+00   \n",
       "max    5.000000e-01      7.071068e-01      1.000000e+00  3.200000e+01   \n",
       "\n",
       "       stops1_ratio  stops2_ratio  shared_2gram        cosine  words_hamming  \\\n",
       "count  2.750086e+06  2.750086e+06  2.750086e+06  2.739171e+06   2.750086e+06   \n",
       "mean   9.544433e-01  9.587843e-01  7.278536e-02  2.970622e-01   1.303406e-01   \n",
       "std    5.095573e-01  5.118105e-01  9.986519e-02  2.630138e-01   1.996098e-01   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   0.000000e+00   \n",
       "25%    6.250000e-01  6.363636e-01  0.000000e+00  0.000000e+00   0.000000e+00   \n",
       "50%    8.571429e-01  8.571429e-01  3.333333e-02  2.676487e-01   2.702703e-02   \n",
       "75%    1.200000e+00  1.200000e+00  1.111111e-01  4.735937e-01   1.818182e-01   \n",
       "max    1.000000e+01  9.000000e+00  5.000000e-01  1.000000e+00   1.000000e+00   \n",
       "\n",
       "       diff_stops_r  ...      who_both      q1_where      q2_where  \\\n",
       "count  2.750086e+06  ...  2.750086e+06  2.750086e+06  2.750086e+06   \n",
       "mean  -4.340919e-03  ...  6.825677e-02  2.500795e-02  2.491122e-02   \n",
       "std    6.139485e-01  ...  2.914741e-01  1.561491e-01  1.558546e-01   \n",
       "min   -8.666667e+00  ...  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%   -3.250000e-01  ...  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  ...  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    3.000000e-01  ...  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    8.400000e+00  ...  2.000000e+00  1.000000e+00  1.000000e+00   \n",
       "\n",
       "         where_both       q1_when       q2_when     when_both        q1_why  \\\n",
       "count  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06  2.750086e+06   \n",
       "mean   4.991917e-02  3.273570e-02  3.223208e-02  6.496779e-02  9.631372e-02   \n",
       "std    2.489788e-01  1.779441e-01  1.766159e-01  2.759281e-01  2.950210e-01   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    2.000000e+00  1.000000e+00  1.000000e+00  2.000000e+00  1.000000e+00   \n",
       "\n",
       "             q2_why      why_both  \n",
       "count  2.750086e+06  2.750086e+06  \n",
       "mean   9.600136e-02  1.923151e-01  \n",
       "std    2.945932e-01  4.697682e-01  \n",
       "min    0.000000e+00  0.000000e+00  \n",
       "25%    0.000000e+00  0.000000e+00  \n",
       "50%    0.000000e+00  0.000000e+00  \n",
       "75%    0.000000e+00  0.000000e+00  \n",
       "max    1.000000e+00  2.000000e+00  \n",
       "\n",
       "[8 rows x 48 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word_match</th>\n      <th>word_match_2root</th>\n      <th>tfidf_word_match</th>\n      <th>shared_count</th>\n      <th>stops1_ratio</th>\n      <th>stops2_ratio</th>\n      <th>shared_2gram</th>\n      <th>cosine</th>\n      <th>words_hamming</th>\n      <th>diff_stops_r</th>\n      <th>...</th>\n      <th>who_both</th>\n      <th>q1_where</th>\n      <th>q2_where</th>\n      <th>where_both</th>\n      <th>q1_when</th>\n      <th>q2_when</th>\n      <th>when_both</th>\n      <th>q1_why</th>\n      <th>q2_why</th>\n      <th>why_both</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2.749809e+06</td>\n      <td>2.749809e+06</td>\n      <td>2.750086e+06</td>\n      <td>2.750086e+06</td>\n      <td>2.750086e+06</td>\n      <td>2.750086e+06</td>\n      <td>2.750086e+06</td>\n      <td>2.739171e+06</td>\n      <td>2.750086e+06</td>\n      <td>2.750086e+06</td>\n      <td>...</td>\n      <td>2.750086e+06</td>\n      <td>2.750086e+06</td>\n      <td>2.750086e+06</td>\n      <td>2.750086e+06</td>\n      <td>2.750086e+06</td>\n      <td>2.750086e+06</td>\n      <td>2.750086e+06</td>\n      <td>2.750086e+06</td>\n      <td>2.750086e+06</td>\n      <td>2.750086e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.473579e-01</td>\n      <td>3.164050e-01</td>\n      <td>2.019946e-01</td>\n      <td>1.600848e+00</td>\n      <td>9.544433e-01</td>\n      <td>9.587843e-01</td>\n      <td>7.278536e-02</td>\n      <td>2.970622e-01</td>\n      <td>1.303406e-01</td>\n      <td>-4.340919e-03</td>\n      <td>...</td>\n      <td>6.825677e-02</td>\n      <td>2.500795e-02</td>\n      <td>2.491122e-02</td>\n      <td>4.991917e-02</td>\n      <td>3.273570e-02</td>\n      <td>3.223208e-02</td>\n      <td>6.496779e-02</td>\n      <td>9.631372e-02</td>\n      <td>9.600136e-02</td>\n      <td>1.923151e-01</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.282001e-01</td>\n      <td>2.173611e-01</td>\n      <td>2.017327e-01</td>\n      <td>1.543970e+00</td>\n      <td>5.095573e-01</td>\n      <td>5.118105e-01</td>\n      <td>9.986519e-02</td>\n      <td>2.630138e-01</td>\n      <td>1.996098e-01</td>\n      <td>6.139485e-01</td>\n      <td>...</td>\n      <td>2.914741e-01</td>\n      <td>1.561491e-01</td>\n      <td>1.558546e-01</td>\n      <td>2.489788e-01</td>\n      <td>1.779441e-01</td>\n      <td>1.766159e-01</td>\n      <td>2.759281e-01</td>\n      <td>2.950210e-01</td>\n      <td>2.945932e-01</td>\n      <td>4.697682e-01</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>-8.666667e+00</td>\n      <td>...</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>6.250000e-01</td>\n      <td>6.363636e-01</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>-3.250000e-01</td>\n      <td>...</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.340544e-01</td>\n      <td>3.661345e-01</td>\n      <td>1.666667e-01</td>\n      <td>1.000000e+00</td>\n      <td>8.571429e-01</td>\n      <td>8.571429e-01</td>\n      <td>3.333333e-02</td>\n      <td>2.676487e-01</td>\n      <td>2.702703e-02</td>\n      <td>0.000000e+00</td>\n      <td>...</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.319422e-01</td>\n      <td>4.816038e-01</td>\n      <td>3.000000e-01</td>\n      <td>2.000000e+00</td>\n      <td>1.200000e+00</td>\n      <td>1.200000e+00</td>\n      <td>1.111111e-01</td>\n      <td>4.735937e-01</td>\n      <td>1.818182e-01</td>\n      <td>3.000000e-01</td>\n      <td>...</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>5.000000e-01</td>\n      <td>7.071068e-01</td>\n      <td>1.000000e+00</td>\n      <td>3.200000e+01</td>\n      <td>1.000000e+01</td>\n      <td>9.000000e+00</td>\n      <td>5.000000e-01</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>8.400000e+00</td>\n      <td>...</td>\n      <td>2.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>2.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>2.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>2.000000e+00</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 48 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Features: ['word_match', 'word_match_2root', 'tfidf_word_match', 'shared_count', 'stops1_ratio', 'stops2_ratio', 'shared_2gram', 'cosine', 'words_hamming', 'diff_stops_r', 'len_q1', 'len_q2', 'diff_len', 'caps_count_q1', 'caps_count_q2', 'diff_caps', 'len_char_q1', 'len_char_q2', 'diff_len_char', 'len_word_q1', 'len_word_q2', 'diff_len_word', 'avg_world_len1', 'avg_world_len2', 'diff_avg_word', 'exactly_same', 'duplicated', 'q1_how', 'q2_how', 'how_both', 'q1_what', 'q2_what', 'what_both', 'q1_which', 'q2_which', 'which_both', 'q1_who', 'q2_who', 'who_both', 'q1_where', 'q2_where', 'where_both', 'q1_when', 'q2_when', 'when_both', 'q1_why', 'q2_why', 'why_both']\n"
     ]
    }
   ],
   "source": [
    "feature_names = list(x.columns.values)\n",
    "print(\"Features: {}\".format(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x[:df_train.shape[0]]\n",
    "x_test  = x[df_train.shape[0]:]\n",
    "y_train = df_train['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = x_train[y_train == 1]\n",
    "neg_train = x_train[y_train == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pos: 149263, neg: 255027\n"
     ]
    }
   ],
   "source": [
    "print(\"pos: {}, neg: {}\".format(len(pos_train), len(neg_train)))"
   ]
  },
  {
   "source": [
    "### Sử dụng oversampling cho negative class có vẻ tốt hơn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversample the negative class\n",
    "# print(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "# p = 0.165\n",
    "# scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "# while scale > 1:\n",
    "#     neg_train = pd.concat([neg_train, neg_train])\n",
    "#     scale -=1\n",
    "# neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "# print(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n",
    "\n",
    "# x_train = pd.concat([pos_train, neg_train])\n",
    "# y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"pos: {}, neg: {}\".format(len(pos_train), len(neg_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data: X_train: (404290, 48), Y_train: 404290, X_test: (2345796, 48)\n"
     ]
    }
   ],
   "source": [
    " print(\"Training data: X_train: {}, Y_train: {}, X_test: {}\".format(x_train.shape, len(y_train), x_test.shape))"
   ]
  },
  {
   "source": [
    "# Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Will train XGB for 190 rounds, RandomSeed: 123457\n",
      "[LightGBM] [Warning] Unknown parameter: boosting)type\n",
      "[LightGBM] [Warning] Unknown parameter: boosting)type\n",
      "[LightGBM] [Info] Number of positive: 119164, number of negative: 204268\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.377128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4639\n",
      "[LightGBM] [Info] Number of data points in the train set: 323432, number of used features: 46\n",
      "[LightGBM] [Warning] Unknown parameter: boosting)type\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.368436 -> initscore=-0.538932\n",
      "[LightGBM] [Info] Start training from score -0.538932\n",
      "[1]\tvalid_0's binary_logloss: 0.61244\n",
      "[2]\tvalid_0's binary_logloss: 0.580625\n",
      "[3]\tvalid_0's binary_logloss: 0.551635\n",
      "[4]\tvalid_0's binary_logloss: 0.52903\n",
      "[5]\tvalid_0's binary_logloss: 0.509478\n",
      "[6]\tvalid_0's binary_logloss: 0.495775\n",
      "[7]\tvalid_0's binary_logloss: 0.482306\n",
      "[8]\tvalid_0's binary_logloss: 0.470577\n",
      "[9]\tvalid_0's binary_logloss: 0.462051\n",
      "[10]\tvalid_0's binary_logloss: 0.453985\n",
      "[11]\tvalid_0's binary_logloss: 0.447038\n",
      "[12]\tvalid_0's binary_logloss: 0.441683\n",
      "[13]\tvalid_0's binary_logloss: 0.436414\n",
      "[14]\tvalid_0's binary_logloss: 0.432095\n",
      "[15]\tvalid_0's binary_logloss: 0.428192\n",
      "[16]\tvalid_0's binary_logloss: 0.425453\n",
      "[17]\tvalid_0's binary_logloss: 0.42285\n",
      "[18]\tvalid_0's binary_logloss: 0.420464\n",
      "[19]\tvalid_0's binary_logloss: 0.418555\n",
      "[20]\tvalid_0's binary_logloss: 0.416598\n",
      "[21]\tvalid_0's binary_logloss: 0.415197\n",
      "[22]\tvalid_0's binary_logloss: 0.413624\n",
      "[23]\tvalid_0's binary_logloss: 0.412153\n",
      "[24]\tvalid_0's binary_logloss: 0.410942\n",
      "[25]\tvalid_0's binary_logloss: 0.410014\n",
      "[26]\tvalid_0's binary_logloss: 0.408951\n",
      "[27]\tvalid_0's binary_logloss: 0.408035\n",
      "[28]\tvalid_0's binary_logloss: 0.40714\n",
      "[29]\tvalid_0's binary_logloss: 0.406326\n",
      "[30]\tvalid_0's binary_logloss: 0.405641\n",
      "[31]\tvalid_0's binary_logloss: 0.404935\n",
      "[32]\tvalid_0's binary_logloss: 0.404348\n",
      "[33]\tvalid_0's binary_logloss: 0.403802\n",
      "[34]\tvalid_0's binary_logloss: 0.40331\n",
      "[35]\tvalid_0's binary_logloss: 0.402794\n",
      "[36]\tvalid_0's binary_logloss: 0.4024\n",
      "[37]\tvalid_0's binary_logloss: 0.402138\n",
      "[38]\tvalid_0's binary_logloss: 0.401828\n",
      "[39]\tvalid_0's binary_logloss: 0.401508\n",
      "[40]\tvalid_0's binary_logloss: 0.4012\n",
      "[41]\tvalid_0's binary_logloss: 0.400821\n",
      "[42]\tvalid_0's binary_logloss: 0.400282\n",
      "[43]\tvalid_0's binary_logloss: 0.399946\n",
      "[44]\tvalid_0's binary_logloss: 0.399755\n",
      "[45]\tvalid_0's binary_logloss: 0.39951\n",
      "[46]\tvalid_0's binary_logloss: 0.39931\n",
      "[47]\tvalid_0's binary_logloss: 0.399069\n",
      "[48]\tvalid_0's binary_logloss: 0.398871\n",
      "[49]\tvalid_0's binary_logloss: 0.398757\n",
      "[50]\tvalid_0's binary_logloss: 0.398422\n",
      "[51]\tvalid_0's binary_logloss: 0.398371\n",
      "[52]\tvalid_0's binary_logloss: 0.39818\n",
      "[53]\tvalid_0's binary_logloss: 0.397982\n",
      "[54]\tvalid_0's binary_logloss: 0.397919\n",
      "[55]\tvalid_0's binary_logloss: 0.397735\n",
      "[56]\tvalid_0's binary_logloss: 0.397637\n",
      "[57]\tvalid_0's binary_logloss: 0.397499\n",
      "[58]\tvalid_0's binary_logloss: 0.397345\n",
      "[59]\tvalid_0's binary_logloss: 0.397292\n",
      "[60]\tvalid_0's binary_logloss: 0.397131\n",
      "[61]\tvalid_0's binary_logloss: 0.397126\n",
      "[62]\tvalid_0's binary_logloss: 0.397081\n",
      "[63]\tvalid_0's binary_logloss: 0.397026\n",
      "[64]\tvalid_0's binary_logloss: 0.396869\n",
      "[65]\tvalid_0's binary_logloss: 0.396786\n",
      "[66]\tvalid_0's binary_logloss: 0.39671\n",
      "[67]\tvalid_0's binary_logloss: 0.396698\n",
      "[68]\tvalid_0's binary_logloss: 0.396652\n",
      "[69]\tvalid_0's binary_logloss: 0.396629\n",
      "[70]\tvalid_0's binary_logloss: 0.396597\n",
      "[71]\tvalid_0's binary_logloss: 0.396494\n",
      "[72]\tvalid_0's binary_logloss: 0.396521\n",
      "[73]\tvalid_0's binary_logloss: 0.3964\n",
      "[74]\tvalid_0's binary_logloss: 0.396418\n",
      "[75]\tvalid_0's binary_logloss: 0.396407\n",
      "[76]\tvalid_0's binary_logloss: 0.396372\n",
      "[77]\tvalid_0's binary_logloss: 0.396335\n",
      "[78]\tvalid_0's binary_logloss: 0.396309\n",
      "[79]\tvalid_0's binary_logloss: 0.396238\n",
      "[80]\tvalid_0's binary_logloss: 0.3962\n",
      "[81]\tvalid_0's binary_logloss: 0.396095\n",
      "[82]\tvalid_0's binary_logloss: 0.395992\n",
      "[83]\tvalid_0's binary_logloss: 0.395888\n",
      "[84]\tvalid_0's binary_logloss: 0.395725\n",
      "[85]\tvalid_0's binary_logloss: 0.395626\n",
      "[86]\tvalid_0's binary_logloss: 0.395564\n",
      "[87]\tvalid_0's binary_logloss: 0.395597\n",
      "[88]\tvalid_0's binary_logloss: 0.395635\n",
      "[89]\tvalid_0's binary_logloss: 0.395635\n",
      "[90]\tvalid_0's binary_logloss: 0.395697\n",
      "[91]\tvalid_0's binary_logloss: 0.395658\n",
      "[92]\tvalid_0's binary_logloss: 0.395619\n",
      "[93]\tvalid_0's binary_logloss: 0.395634\n",
      "[94]\tvalid_0's binary_logloss: 0.395599\n",
      "[95]\tvalid_0's binary_logloss: 0.395573\n",
      "[96]\tvalid_0's binary_logloss: 0.395517\n",
      "[97]\tvalid_0's binary_logloss: 0.395517\n",
      "[98]\tvalid_0's binary_logloss: 0.395452\n",
      "[99]\tvalid_0's binary_logloss: 0.39546\n",
      "[100]\tvalid_0's binary_logloss: 0.395464\n",
      "[101]\tvalid_0's binary_logloss: 0.395466\n",
      "[102]\tvalid_0's binary_logloss: 0.395409\n",
      "[103]\tvalid_0's binary_logloss: 0.395429\n",
      "[104]\tvalid_0's binary_logloss: 0.395371\n",
      "[105]\tvalid_0's binary_logloss: 0.395407\n",
      "[106]\tvalid_0's binary_logloss: 0.3954\n",
      "[107]\tvalid_0's binary_logloss: 0.395401\n",
      "[108]\tvalid_0's binary_logloss: 0.395422\n",
      "[109]\tvalid_0's binary_logloss: 0.395376\n",
      "[110]\tvalid_0's binary_logloss: 0.395425\n",
      "[111]\tvalid_0's binary_logloss: 0.395365\n",
      "[112]\tvalid_0's binary_logloss: 0.395363\n",
      "[113]\tvalid_0's binary_logloss: 0.395298\n",
      "[114]\tvalid_0's binary_logloss: 0.395372\n",
      "[115]\tvalid_0's binary_logloss: 0.395437\n",
      "[116]\tvalid_0's binary_logloss: 0.395371\n",
      "[117]\tvalid_0's binary_logloss: 0.395254\n",
      "[118]\tvalid_0's binary_logloss: 0.395237\n",
      "[119]\tvalid_0's binary_logloss: 0.394993\n",
      "[120]\tvalid_0's binary_logloss: 0.395032\n",
      "[121]\tvalid_0's binary_logloss: 0.395013\n",
      "[122]\tvalid_0's binary_logloss: 0.39497\n",
      "[123]\tvalid_0's binary_logloss: 0.394996\n",
      "[124]\tvalid_0's binary_logloss: 0.39496\n",
      "[125]\tvalid_0's binary_logloss: 0.394931\n",
      "[126]\tvalid_0's binary_logloss: 0.39487\n",
      "[127]\tvalid_0's binary_logloss: 0.394856\n",
      "[128]\tvalid_0's binary_logloss: 0.394889\n",
      "[129]\tvalid_0's binary_logloss: 0.394853\n",
      "[130]\tvalid_0's binary_logloss: 0.394856\n",
      "[131]\tvalid_0's binary_logloss: 0.394828\n",
      "[132]\tvalid_0's binary_logloss: 0.394854\n",
      "[133]\tvalid_0's binary_logloss: 0.39484\n",
      "[134]\tvalid_0's binary_logloss: 0.394815\n",
      "[135]\tvalid_0's binary_logloss: 0.39477\n",
      "[136]\tvalid_0's binary_logloss: 0.394724\n",
      "[137]\tvalid_0's binary_logloss: 0.394728\n",
      "[138]\tvalid_0's binary_logloss: 0.394753\n",
      "[139]\tvalid_0's binary_logloss: 0.394803\n",
      "[140]\tvalid_0's binary_logloss: 0.394762\n",
      "[141]\tvalid_0's binary_logloss: 0.394851\n",
      "[142]\tvalid_0's binary_logloss: 0.394852\n",
      "[143]\tvalid_0's binary_logloss: 0.394769\n",
      "[144]\tvalid_0's binary_logloss: 0.394797\n",
      "[145]\tvalid_0's binary_logloss: 0.394876\n",
      "[146]\tvalid_0's binary_logloss: 0.394777\n",
      "[147]\tvalid_0's binary_logloss: 0.394796\n",
      "[148]\tvalid_0's binary_logloss: 0.394854\n",
      "[149]\tvalid_0's binary_logloss: 0.394841\n",
      "[150]\tvalid_0's binary_logloss: 0.394883\n",
      "[151]\tvalid_0's binary_logloss: 0.394918\n",
      "[152]\tvalid_0's binary_logloss: 0.394976\n",
      "[153]\tvalid_0's binary_logloss: 0.394997\n",
      "[154]\tvalid_0's binary_logloss: 0.394981\n",
      "[155]\tvalid_0's binary_logloss: 0.395049\n",
      "[156]\tvalid_0's binary_logloss: 0.395075\n",
      "[157]\tvalid_0's binary_logloss: 0.395191\n",
      "[158]\tvalid_0's binary_logloss: 0.395153\n",
      "[159]\tvalid_0's binary_logloss: 0.395102\n",
      "[160]\tvalid_0's binary_logloss: 0.395149\n",
      "[161]\tvalid_0's binary_logloss: 0.395185\n",
      "[162]\tvalid_0's binary_logloss: 0.39513\n",
      "[163]\tvalid_0's binary_logloss: 0.395145\n",
      "[164]\tvalid_0's binary_logloss: 0.395102\n",
      "[165]\tvalid_0's binary_logloss: 0.39507\n",
      "[166]\tvalid_0's binary_logloss: 0.395126\n",
      "[167]\tvalid_0's binary_logloss: 0.395185\n",
      "[168]\tvalid_0's binary_logloss: 0.395239\n",
      "[169]\tvalid_0's binary_logloss: 0.395233\n",
      "[170]\tvalid_0's binary_logloss: 0.395203\n",
      "[171]\tvalid_0's binary_logloss: 0.395194\n",
      "[172]\tvalid_0's binary_logloss: 0.395275\n",
      "[173]\tvalid_0's binary_logloss: 0.395302\n",
      "[174]\tvalid_0's binary_logloss: 0.395297\n",
      "[175]\tvalid_0's binary_logloss: 0.395282\n",
      "[176]\tvalid_0's binary_logloss: 0.395335\n",
      "[177]\tvalid_0's binary_logloss: 0.395351\n",
      "[178]\tvalid_0's binary_logloss: 0.395395\n",
      "[179]\tvalid_0's binary_logloss: 0.395506\n",
      "[180]\tvalid_0's binary_logloss: 0.395492\n",
      "[181]\tvalid_0's binary_logloss: 0.395545\n",
      "[182]\tvalid_0's binary_logloss: 0.395454\n",
      "[183]\tvalid_0's binary_logloss: 0.395479\n",
      "[184]\tvalid_0's binary_logloss: 0.39548\n",
      "[185]\tvalid_0's binary_logloss: 0.395536\n",
      "[186]\tvalid_0's binary_logloss: 0.39552\n",
      "[187]\tvalid_0's binary_logloss: 0.395513\n",
      "[188]\tvalid_0's binary_logloss: 0.395504\n",
      "[189]\tvalid_0's binary_logloss: 0.395476\n",
      "[190]\tvalid_0's binary_logloss: 0.395534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "RS = 123457\n",
    "ROUNDS = 190\n",
    "def train_xgb(X, y, params):\n",
    "    print(\"Will train XGB for {} rounds, RandomSeed: {}\".format(ROUNDS, RS))\n",
    "    x, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RS)\n",
    "    xg_train = lgb.Dataset(x, label=y_train)\n",
    "    xg_val = lgb.Dataset(X_val, label=y_val)\n",
    "    watchlist  = [xg_val]\n",
    "    clf = lgb.train(params, xg_train, ROUNDS, watchlist)\n",
    "    return clf\n",
    "\n",
    "def predict_xgb(clr, X_test):\n",
    "    return clr.predict(X_test)\n",
    "\n",
    "clr = train_xgb(x_train.fillna(0), y_train, param)\n",
    "preds = predict_xgb(clr, x_test.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Writing output...\")\n",
    "# sub = pd.DataFrame()\n",
    "# sub['test_id'] = df_test['test_id']\n",
    "# sub['is_duplicate'] = preds *.75\n",
    "# sub.to_csv(\"lgb_seed{}_n{}.csv\".format(RS, ROUNDS), index=False)"
   ]
  }
 ]
}