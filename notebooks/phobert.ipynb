{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 28091997\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             question1  \\\n",
       "0    Trong tình huống nào thì người bệnh u máu gan ...   \n",
       "1    Trong tình huống nào thì người bệnh u máu gan ...   \n",
       "2    Triệu chứng lâm sàng của bệnh u máu gan là gì ...   \n",
       "3    Triệu chứng lâm sàng của bệnh u máu gan là gì ...   \n",
       "4    Rượu vang trắng có liên quan tới ung thư da ác...   \n",
       "..                                                 ...   \n",
       "128  Bổ sung vitamin E sau sinh như thế nào là hiệu...   \n",
       "129   Khi nào người bệnh u máu gan sẽ được phẫu thuật?   \n",
       "130  Ung thư vòm họng giai đoạn cuối biểu hiện như ...   \n",
       "131          Những loại đồ ăn nào bà bầu không nên ăn?   \n",
       "132                    Cách tắm bé tại nhà chuẩn nhất?   \n",
       "\n",
       "                                             question2  label  \n",
       "0      Người bệnh u máu gan được cấy ghép gan khi nào?      1  \n",
       "1     Khi nào người bệnh u máu gan sẽ được phẫu thuật?      1  \n",
       "2                  Các triệu chứng của bệnh u máu gan?      1  \n",
       "3                              Dấu hiệu của u máu gan?      1  \n",
       "4    Rượu vang trắng có liên quan tới ung thư da ác...      1  \n",
       "..                                                 ...    ...  \n",
       "128                            Dấu hiệu của u máu gan?      0  \n",
       "129          Mẹ bầu cần tránh ăn những loại đồ ăn nào?      0  \n",
       "130           Thận yếu ở phụ nữ biểu hiện như thế nào?      0  \n",
       "131          Lời khuyên từ chuyên gia khi tắm cho trẻ?      0  \n",
       "132                            Dấu hiệu của u máu gan?      0  \n",
       "\n",
       "[133 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Trong tình huống nào thì người bệnh u máu gan ...</td>\n      <td>Người bệnh u máu gan được cấy ghép gan khi nào?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Trong tình huống nào thì người bệnh u máu gan ...</td>\n      <td>Khi nào người bệnh u máu gan sẽ được phẫu thuật?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Triệu chứng lâm sàng của bệnh u máu gan là gì ...</td>\n      <td>Các triệu chứng của bệnh u máu gan?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Triệu chứng lâm sàng của bệnh u máu gan là gì ...</td>\n      <td>Dấu hiệu của u máu gan?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Rượu vang trắng có liên quan tới ung thư da ác...</td>\n      <td>Rượu vang trắng có liên quan tới ung thư da ác...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>Bổ sung vitamin E sau sinh như thế nào là hiệu...</td>\n      <td>Dấu hiệu của u máu gan?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>Khi nào người bệnh u máu gan sẽ được phẫu thuật?</td>\n      <td>Mẹ bầu cần tránh ăn những loại đồ ăn nào?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>Ung thư vòm họng giai đoạn cuối biểu hiện như ...</td>\n      <td>Thận yếu ở phụ nữ biểu hiện như thế nào?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>Những loại đồ ăn nào bà bầu không nên ăn?</td>\n      <td>Lời khuyên từ chuyên gia khi tắm cho trẻ?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>Cách tắm bé tại nhà chuẩn nhất?</td>\n      <td>Dấu hiệu của u máu gan?</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>133 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/processed/train.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 133 entries, 0 to 132\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   question1  133 non-null    object\n 1   question2  133 non-null    object\n 2   label      133 non-null    int64 \ndtypes: int64(1), object(2)\nmemory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHOBERT_VERSION = \"vinai/phobert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PHOBERT_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(data, test_size = 0.2)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoBertDataset:\n",
    "    def __init__(self, first_questions, second_questions, targets, tokenizer):\n",
    "        self.first_questions = first_questions\n",
    "        self.second_questions = second_questions\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.length = len(first_questions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        first_questions = str(self.first_questions[item])\n",
    "        second_questions = str(self.second_questions[item])\n",
    "\n",
    "        # remove extra white spaces from questions\n",
    "        first_questions = \" \".join(first_questions.split())\n",
    "        second_questions = \" \".join(second_questions.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            first_questions,\n",
    "            second_questions,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            max_length=2*MAX_LEN+3, # max length of 2 questions and 3 spectial tokens\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # return targets 0, when using data set in testing and targets are none\n",
    "        return {\n",
    "            \"ids\": torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n",
    "            \"mask\": torch.tensor(inputs[\"attention_mask\"], dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long),\n",
    "            \"targets\": torch.tensor(int(self.targets[item]), dtype=torch.long) if self.targets is not None else 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates dataset and returns dataloader of it\n",
    "def get_data_loader(df, targets, batch_size, shuffle, tokenizer):\n",
    "    dataset = PhoBertDataset(\n",
    "        first_questions=df[\"question1\"].values,\n",
    "        second_questions=df[\"question2\"].values,\n",
    "        targets=targets,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders of training and validation data\n",
    "train_data_loader = get_data_loader(\n",
    "    df=train_df,\n",
    "    targets=train_df[\"label\"].values,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "val_data_loader = get_data_loader(\n",
    "    df=val_df,\n",
    "    targets=val_df[\"label\"].values,\n",
    "    batch_size=4*BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "source": [
    "# Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoBertModel(nn.Module):\n",
    "    def __init__(self, phobert_path):\n",
    "        super(PhoBertModel, self).__init__()\n",
    "        self.phobert_path = phobert_path\n",
    "        self.phobert = AutoModel.from_pretrained(self.phobert_path)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, pooled = self.phobert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        #add dropout to prevent overfitting\n",
    "        pooled = self.dropout(pooled)\n",
    "        return self.out(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PhoBertModel(PHOBERT_VERSION).to(device)"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function = binary cross entropy loss\n",
    "# using sigmoid to put probabilities in [0,1] interval\n",
    "def loss_fn(outputs, targets):\n",
    "    outputs = torch.squeeze(outputs)\n",
    "    return nn.BCELoss()(nn.Sigmoid()(outputs), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(data_loader, model, device):\n",
    "    model.eval()\n",
    "    # tells Pytorch not to store values of intermediate computations for backward pass because we not gonna need gradients\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for batch in data_loader:\n",
    "            ids = batch[\"ids\"].to(device, dtype=torch.long)\n",
    "            mask = batch[\"mask\"].to(device, dtype=torch.long)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            targets = batch[\"targets\"].to(device, dtype=torch.float)\n",
    "\n",
    "            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "            total_loss += loss_fn(outputs, targets).item()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return np.exp(total_loss/len(data_loader)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epochs, train_data_loader, val_data_loader, model, optimizer, device, scheduler=None):\n",
    "    it = 1\n",
    "    total_loss = 0\n",
    "    curr_perplexity = None\n",
    "    perplexity = None\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch: ', epoch + 1)\n",
    "        for batch in train_data_loader:\n",
    "            ids = batch[\"ids\"].to(device, dtype=torch.long)\n",
    "            mask = batch[\"mask\"].to(device, dtype=torch.long)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            targets = batch[\"targets\"].to(device, dtype=torch.float)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # do forward pass, will save intermediate computations of the graph for later backprop use.\n",
    "            outputs = model(ids, mask=mask, token_type_ids=token_type_ids)\n",
    "            \n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # running backprop.\n",
    "            loss.backward()\n",
    "            \n",
    "            # doing gradient descent step.\n",
    "            optimizer.step()\n",
    "            \n",
    "            # we are logging current loss/perplexity in every 100 iteration\n",
    "            if it % 5 == 0:\n",
    "                \n",
    "                # computing validation set perplexity in every 500 iteration.\n",
    "                if it % 20 == 0:\n",
    "                    curr_perplexity = calculate_perplexity(val_data_loader, model, device)\n",
    "                    \n",
    "                    if scheduler is not None:\n",
    "                        scheduler.step()\n",
    "\n",
    "                    # making checkpoint of best model weights.\n",
    "                    if not perplexity or curr_perplexity < perplexity:\n",
    "                        torch.save(model.state_dict(), 'saved_model')\n",
    "                        perplexity = curr_perplexity\n",
    "\n",
    "                print('| Iter', it, '| Avg Train Loss', total_loss / 100, '| Dev Perplexity', curr_perplexity)\n",
    "                total_loss = 0\n",
    "\n",
    "            it += 1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, train_df, device, train_data_loader, val_data_loader):\n",
    "    EPOCHS = 1000\n",
    "    \n",
    "    lr = 3e-5\n",
    "    num_training_steps = int(len(train_data_loader) * EPOCHS)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    \n",
    "    train_loop(EPOCHS, train_data_loader, val_data_loader,  model, optimizer, device, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ev Perplexity 2.020630507526752\n",
      "| Iter 104860 | Avg Train Loss 0.035775671601295474 | Dev Perplexity 2.0211818834941426\n",
      "| Iter 104865 | Avg Train Loss 0.035271175503730774 | Dev Perplexity 2.0211818834941426\n",
      "| Iter 104870 | Avg Train Loss 0.035245587229728696 | Dev Perplexity 2.0211818834941426\n",
      "| Iter 104875 | Avg Train Loss 0.03442310273647308 | Dev Perplexity 2.0211818834941426\n",
      "| Iter 104880 | Avg Train Loss 0.03566416323184967 | Dev Perplexity 2.008531035512192\n",
      "| Iter 104885 | Avg Train Loss 0.03363487601280212 | Dev Perplexity 2.008531035512192\n",
      "| Iter 104890 | Avg Train Loss 0.0348351639509201 | Dev Perplexity 2.008531035512192\n",
      "| Iter 104895 | Avg Train Loss 0.03455220997333527 | Dev Perplexity 2.008531035512192\n",
      "| Iter 104900 | Avg Train Loss 0.03473040997982025 | Dev Perplexity 2.0063283766815374\n",
      "| Iter 104905 | Avg Train Loss 0.03393917739391327 | Dev Perplexity 2.0063283766815374\n",
      "| Iter 104910 | Avg Train Loss 0.03392826735973358 | Dev Perplexity 2.0063283766815374\n",
      "| Iter 104915 | Avg Train Loss 0.034156026244163515 | Dev Perplexity 2.0063283766815374\n",
      "| Iter 104920 | Avg Train Loss 0.03491699874401093 | Dev Perplexity 2.0139061892762427\n",
      "| Iter 104925 | Avg Train Loss 0.03328969895839691 | Dev Perplexity 2.0139061892762427\n",
      "| Iter 104930 | Avg Train Loss 0.031297757029533385 | Dev Perplexity 2.0139061892762427\n",
      "| Iter 104935 | Avg Train Loss 0.035425556302070615 | Dev Perplexity 2.0139061892762427\n",
      "| Iter 104940 | Avg Train Loss 0.03373726427555084 | Dev Perplexity 2.029673176811595\n",
      "Epoch:  991\n",
      "| Iter 104945 | Avg Train Loss 0.03603727102279663 | Dev Perplexity 2.029673176811595\n",
      "| Iter 104950 | Avg Train Loss 0.032532050013542174 | Dev Perplexity 2.029673176811595\n",
      "| Iter 104955 | Avg Train Loss 0.03766664683818817 | Dev Perplexity 2.029673176811595\n",
      "| Iter 104960 | Avg Train Loss 0.032881682515144346 | Dev Perplexity 2.0254786834182616\n",
      "| Iter 104965 | Avg Train Loss 0.034135901927948 | Dev Perplexity 2.0254786834182616\n",
      "| Iter 104970 | Avg Train Loss 0.032297624349594115 | Dev Perplexity 2.0254786834182616\n",
      "| Iter 104975 | Avg Train Loss 0.03612470984458924 | Dev Perplexity 2.0254786834182616\n",
      "| Iter 104980 | Avg Train Loss 0.03576634109020233 | Dev Perplexity 2.0299855659127735\n",
      "| Iter 104985 | Avg Train Loss 0.03387357592582703 | Dev Perplexity 2.0299855659127735\n",
      "| Iter 104990 | Avg Train Loss 0.03365483105182648 | Dev Perplexity 2.0299855659127735\n",
      "| Iter 104995 | Avg Train Loss 0.03586764216423035 | Dev Perplexity 2.0299855659127735\n",
      "| Iter 105000 | Avg Train Loss 0.03751114785671234 | Dev Perplexity 2.02796021284684\n",
      "| Iter 105005 | Avg Train Loss 0.0354960435628891 | Dev Perplexity 2.02796021284684\n",
      "| Iter 105010 | Avg Train Loss 0.03365236878395081 | Dev Perplexity 2.02796021284684\n",
      "| Iter 105015 | Avg Train Loss 0.03537684619426727 | Dev Perplexity 2.02796021284684\n",
      "| Iter 105020 | Avg Train Loss 0.032385211586952206 | Dev Perplexity 2.0163292270585984\n",
      "| Iter 105025 | Avg Train Loss 0.03399271309375763 | Dev Perplexity 2.0163292270585984\n",
      "| Iter 105030 | Avg Train Loss 0.034239100813865664 | Dev Perplexity 2.0163292270585984\n",
      "| Iter 105035 | Avg Train Loss 0.03277029812335968 | Dev Perplexity 2.0163292270585984\n",
      "| Iter 105040 | Avg Train Loss 0.035373939871788024 | Dev Perplexity 2.0295028470072287\n",
      "| Iter 105045 | Avg Train Loss 0.03474847197532654 | Dev Perplexity 2.0295028470072287\n",
      "Epoch:  992\n",
      "| Iter 105050 | Avg Train Loss 0.03412869274616241 | Dev Perplexity 2.0295028470072287\n",
      "| Iter 105055 | Avg Train Loss 0.034142462015151975 | Dev Perplexity 2.0295028470072287\n",
      "| Iter 105060 | Avg Train Loss 0.0325426572561264 | Dev Perplexity 2.028762106311435\n",
      "| Iter 105065 | Avg Train Loss 0.035016301274299624 | Dev Perplexity 2.028762106311435\n",
      "| Iter 105070 | Avg Train Loss 0.036666340231895446 | Dev Perplexity 2.028762106311435\n",
      "| Iter 105075 | Avg Train Loss 0.0334978199005127 | Dev Perplexity 2.028762106311435\n",
      "| Iter 105080 | Avg Train Loss 0.03093654811382294 | Dev Perplexity 2.0294603704741387\n",
      "| Iter 105085 | Avg Train Loss 0.032293081283569336 | Dev Perplexity 2.0294603704741387\n",
      "| Iter 105090 | Avg Train Loss 0.03598423182964325 | Dev Perplexity 2.0294603704741387\n",
      "| Iter 105095 | Avg Train Loss 0.03533899188041687 | Dev Perplexity 2.0294603704741387\n",
      "| Iter 105100 | Avg Train Loss 0.03230479300022125 | Dev Perplexity 2.0454066237592916\n",
      "| Iter 105105 | Avg Train Loss 0.03583600461483002 | Dev Perplexity 2.0454066237592916\n",
      "| Iter 105110 | Avg Train Loss 0.036923114657402036 | Dev Perplexity 2.0454066237592916\n",
      "| Iter 105115 | Avg Train Loss 0.03668121039867401 | Dev Perplexity 2.0454066237592916\n",
      "| Iter 105120 | Avg Train Loss 0.03744944274425507 | Dev Perplexity 2.036253354925673\n",
      "| Iter 105125 | Avg Train Loss 0.034475384950637816 | Dev Perplexity 2.036253354925673\n",
      "| Iter 105130 | Avg Train Loss 0.03549674689769745 | Dev Perplexity 2.036253354925673\n",
      "| Iter 105135 | Avg Train Loss 0.03503680229187012 | Dev Perplexity 2.036253354925673\n",
      "| Iter 105140 | Avg Train Loss 0.03446629106998444 | Dev Perplexity 2.021051124159657\n",
      "| Iter 105145 | Avg Train Loss 0.03369828462600708 | Dev Perplexity 2.021051124159657\n",
      "| Iter 105150 | Avg Train Loss 0.032357249855995175 | Dev Perplexity 2.021051124159657\n",
      "Epoch:  993\n",
      "| Iter 105155 | Avg Train Loss 0.034254742860794066 | Dev Perplexity 2.021051124159657\n",
      "| Iter 105160 | Avg Train Loss 0.033027251958847044 | Dev Perplexity 2.0258570628654144\n",
      "| Iter 105165 | Avg Train Loss 0.032071656584739684 | Dev Perplexity 2.0258570628654144\n",
      "| Iter 105170 | Avg Train Loss 0.037486757040023806 | Dev Perplexity 2.0258570628654144\n",
      "| Iter 105175 | Avg Train Loss 0.03564468443393707 | Dev Perplexity 2.0258570628654144\n",
      "| Iter 105180 | Avg Train Loss 0.03635581374168396 | Dev Perplexity 2.0248138058413243\n",
      "| Iter 105185 | Avg Train Loss 0.03574825882911682 | Dev Perplexity 2.0248138058413243\n",
      "| Iter 105190 | Avg Train Loss 0.033668359518051146 | Dev Perplexity 2.0248138058413243\n",
      "| Iter 105195 | Avg Train Loss 0.033217355012893676 | Dev Perplexity 2.0248138058413243\n",
      "| Iter 105200 | Avg Train Loss 0.03269804835319519 | Dev Perplexity 2.0267970819556935\n",
      "| Iter 105205 | Avg Train Loss 0.03667078733444214 | Dev Perplexity 2.0267970819556935\n",
      "| Iter 105210 | Avg Train Loss 0.03434735238552093 | Dev Perplexity 2.0267970819556935\n",
      "| Iter 105215 | Avg Train Loss 0.036108702421188354 | Dev Perplexity 2.0267970819556935\n",
      "| Iter 105220 | Avg Train Loss 0.034350000619888306 | Dev Perplexity 2.026245226624589\n",
      "| Iter 105225 | Avg Train Loss 0.034684107899665834 | Dev Perplexity 2.026245226624589\n",
      "| Iter 105230 | Avg Train Loss 0.03622082054615021 | Dev Perplexity 2.026245226624589\n",
      "| Iter 105235 | Avg Train Loss 0.03300396144390106 | Dev Perplexity 2.026245226624589\n",
      "| Iter 105240 | Avg Train Loss 0.036507842540740965 | Dev Perplexity 2.0213225822761536\n",
      "| Iter 105245 | Avg Train Loss 0.03357991576194763 | Dev Perplexity 2.0213225822761536\n",
      "| Iter 105250 | Avg Train Loss 0.03518372654914856 | Dev Perplexity 2.0213225822761536\n",
      "| Iter 105255 | Avg Train Loss 0.03230838775634766 | Dev Perplexity 2.0213225822761536\n",
      "Epoch:  994\n",
      "| Iter 105260 | Avg Train Loss 0.0358364337682724 | Dev Perplexity 2.0220438538613403\n",
      "| Iter 105265 | Avg Train Loss 0.0333795040845871 | Dev Perplexity 2.0220438538613403\n",
      "| Iter 105270 | Avg Train Loss 0.03298234760761261 | Dev Perplexity 2.0220438538613403\n",
      "| Iter 105275 | Avg Train Loss 0.034390222430229184 | Dev Perplexity 2.0220438538613403\n",
      "| Iter 105280 | Avg Train Loss 0.03453023612499237 | Dev Perplexity 2.0368036868992463\n",
      "| Iter 105285 | Avg Train Loss 0.03246329188346863 | Dev Perplexity 2.0368036868992463\n",
      "| Iter 105290 | Avg Train Loss 0.03825980961322784 | Dev Perplexity 2.0368036868992463\n",
      "| Iter 105295 | Avg Train Loss 0.031707889437675475 | Dev Perplexity 2.0368036868992463\n",
      "| Iter 105300 | Avg Train Loss 0.03449226915836334 | Dev Perplexity 2.034473910092417\n",
      "| Iter 105305 | Avg Train Loss 0.03624793410301209 | Dev Perplexity 2.034473910092417\n",
      "| Iter 105310 | Avg Train Loss 0.03620177686214447 | Dev Perplexity 2.034473910092417\n",
      "| Iter 105315 | Avg Train Loss 0.037039603590965274 | Dev Perplexity 2.034473910092417\n",
      "| Iter 105320 | Avg Train Loss 0.03292755663394928 | Dev Perplexity 2.0284468828615645\n",
      "| Iter 105325 | Avg Train Loss 0.03387861728668213 | Dev Perplexity 2.0284468828615645\n",
      "| Iter 105330 | Avg Train Loss 0.03328842759132385 | Dev Perplexity 2.0284468828615645\n",
      "| Iter 105335 | Avg Train Loss 0.034231718182563785 | Dev Perplexity 2.0284468828615645\n",
      "| Iter 105340 | Avg Train Loss 0.03773774027824402 | Dev Perplexity 2.031636630180622\n",
      "| Iter 105345 | Avg Train Loss 0.035656597018241885 | Dev Perplexity 2.031636630180622\n",
      "| Iter 105350 | Avg Train Loss 0.03182211339473724 | Dev Perplexity 2.031636630180622\n",
      "| Iter 105355 | Avg Train Loss 0.03520563006401062 | Dev Perplexity 2.031636630180622\n",
      "| Iter 105360 | Avg Train Loss 0.034998772144317625 | Dev Perplexity 2.0346673008675857\n",
      "Epoch:  995\n",
      "| Iter 105365 | Avg Train Loss 0.03646457731723785 | Dev Perplexity 2.0346673008675857\n",
      "| Iter 105370 | Avg Train Loss 0.03451067805290222 | Dev Perplexity 2.0346673008675857\n",
      "| Iter 105375 | Avg Train Loss 0.03717632412910461 | Dev Perplexity 2.0346673008675857\n",
      "| Iter 105380 | Avg Train Loss 0.03344033420085907 | Dev Perplexity 2.024881564839196\n",
      "| Iter 105385 | Avg Train Loss 0.03500055193901062 | Dev Perplexity 2.024881564839196\n",
      "| Iter 105390 | Avg Train Loss 0.03334778130054474 | Dev Perplexity 2.024881564839196\n",
      "| Iter 105395 | Avg Train Loss 0.035157257318496705 | Dev Perplexity 2.024881564839196\n",
      "| Iter 105400 | Avg Train Loss 0.03392675220966339 | Dev Perplexity 2.0265839041756295\n",
      "| Iter 105405 | Avg Train Loss 0.03148769736289978 | Dev Perplexity 2.0265839041756295\n",
      "| Iter 105410 | Avg Train Loss 0.03573691189289093 | Dev Perplexity 2.0265839041756295\n",
      "| Iter 105415 | Avg Train Loss 0.034201984405517576 | Dev Perplexity 2.0265839041756295\n",
      "| Iter 105420 | Avg Train Loss 0.034492198824882504 | Dev Perplexity 2.0312401174193804\n",
      "| Iter 105425 | Avg Train Loss 0.03578218698501587 | Dev Perplexity 2.0312401174193804\n",
      "| Iter 105430 | Avg Train Loss 0.03750852108001709 | Dev Perplexity 2.0312401174193804\n",
      "| Iter 105435 | Avg Train Loss 0.03367573857307434 | Dev Perplexity 2.0312401174193804\n",
      "| Iter 105440 | Avg Train Loss 0.034247723817825315 | Dev Perplexity 2.031820927799043\n",
      "| Iter 105445 | Avg Train Loss 0.032276500463485715 | Dev Perplexity 2.031820927799043\n",
      "| Iter 105450 | Avg Train Loss 0.034267224073410034 | Dev Perplexity 2.031820927799043\n",
      "| Iter 105455 | Avg Train Loss 0.033974968791007996 | Dev Perplexity 2.031820927799043\n",
      "| Iter 105460 | Avg Train Loss 0.0400324559211731 | Dev Perplexity 2.038210643252547\n",
      "| Iter 105465 | Avg Train Loss 0.0372524756193161 | Dev Perplexity 2.038210643252547\n",
      "| Iter 105470 | Avg Train Loss 0.035964266657829286 | Dev Perplexity 2.038210643252547\n",
      "Epoch:  996\n",
      "| Iter 105475 | Avg Train Loss 0.03366675019264221 | Dev Perplexity 2.038210643252547\n",
      "| Iter 105480 | Avg Train Loss 0.0365329384803772 | Dev Perplexity 2.020569962046282\n",
      "| Iter 105485 | Avg Train Loss 0.033915666341781614 | Dev Perplexity 2.020569962046282\n",
      "| Iter 105490 | Avg Train Loss 0.03470978260040283 | Dev Perplexity 2.020569962046282\n",
      "| Iter 105495 | Avg Train Loss 0.03534574449062347 | Dev Perplexity 2.020569962046282\n",
      "| Iter 105500 | Avg Train Loss 0.03576004683971405 | Dev Perplexity 2.018102579409829\n",
      "| Iter 105505 | Avg Train Loss 0.034629473686218264 | Dev Perplexity 2.018102579409829\n",
      "| Iter 105510 | Avg Train Loss 0.03521113514900207 | Dev Perplexity 2.018102579409829\n",
      "| Iter 105515 | Avg Train Loss 0.03406079113483429 | Dev Perplexity 2.018102579409829\n",
      "| Iter 105520 | Avg Train Loss 0.03317603290081024 | Dev Perplexity 2.016904159494749\n",
      "| Iter 105525 | Avg Train Loss 0.03213912069797516 | Dev Perplexity 2.016904159494749\n",
      "| Iter 105530 | Avg Train Loss 0.03657430768013001 | Dev Perplexity 2.016904159494749\n",
      "| Iter 105535 | Avg Train Loss 0.03443998217582703 | Dev Perplexity 2.016904159494749\n",
      "| Iter 105540 | Avg Train Loss 0.035926149487495423 | Dev Perplexity 2.020107388395371\n",
      "| Iter 105545 | Avg Train Loss 0.034766364693641666 | Dev Perplexity 2.020107388395371\n",
      "| Iter 105550 | Avg Train Loss 0.034012972712516784 | Dev Perplexity 2.020107388395371\n",
      "| Iter 105555 | Avg Train Loss 0.0344512939453125 | Dev Perplexity 2.020107388395371\n",
      "| Iter 105560 | Avg Train Loss 0.03417322337627411 | Dev Perplexity 2.0275069790711533\n",
      "| Iter 105565 | Avg Train Loss 0.034498772621154784 | Dev Perplexity 2.0275069790711533\n",
      "| Iter 105570 | Avg Train Loss 0.034193934202194215 | Dev Perplexity 2.0275069790711533\n",
      "| Iter 105575 | Avg Train Loss 0.03478919327259064 | Dev Perplexity 2.0275069790711533\n",
      "Epoch:  997\n",
      "| Iter 105580 | Avg Train Loss 0.033772445917129516 | Dev Perplexity 2.0249494640433063\n",
      "| Iter 105585 | Avg Train Loss 0.03319493055343628 | Dev Perplexity 2.0249494640433063\n",
      "| Iter 105590 | Avg Train Loss 0.036671690940856934 | Dev Perplexity 2.0249494640433063\n",
      "| Iter 105595 | Avg Train Loss 0.03276045620441437 | Dev Perplexity 2.0249494640433063\n",
      "| Iter 105600 | Avg Train Loss 0.03377846956253052 | Dev Perplexity 2.028341180227449\n",
      "| Iter 105605 | Avg Train Loss 0.03514708399772644 | Dev Perplexity 2.028341180227449\n",
      "| Iter 105610 | Avg Train Loss 0.033410764336586 | Dev Perplexity 2.028341180227449\n",
      "| Iter 105615 | Avg Train Loss 0.03232163488864899 | Dev Perplexity 2.028341180227449\n",
      "| Iter 105620 | Avg Train Loss 0.03512086749076843 | Dev Perplexity 2.0352474448586078\n",
      "| Iter 105625 | Avg Train Loss 0.036989631056785585 | Dev Perplexity 2.0352474448586078\n",
      "| Iter 105630 | Avg Train Loss 0.03566703736782074 | Dev Perplexity 2.0352474448586078\n",
      "| Iter 105635 | Avg Train Loss 0.03367432236671448 | Dev Perplexity 2.0352474448586078\n",
      "| Iter 105640 | Avg Train Loss 0.03291914463043213 | Dev Perplexity 2.029821795021158\n",
      "| Iter 105645 | Avg Train Loss 0.03780084729194641 | Dev Perplexity 2.029821795021158\n",
      "| Iter 105650 | Avg Train Loss 0.035967395901679994 | Dev Perplexity 2.029821795021158\n",
      "| Iter 105655 | Avg Train Loss 0.03414315402507782 | Dev Perplexity 2.029821795021158\n",
      "| Iter 105660 | Avg Train Loss 0.03419704794883728 | Dev Perplexity 2.025610315640837\n",
      "| Iter 105665 | Avg Train Loss 0.033888725638389586 | Dev Perplexity 2.025610315640837\n",
      "| Iter 105670 | Avg Train Loss 0.038156578540802004 | Dev Perplexity 2.025610315640837\n",
      "| Iter 105675 | Avg Train Loss 0.032775638103485105 | Dev Perplexity 2.025610315640837\n",
      "| Iter 105680 | Avg Train Loss 0.03237051784992218 | Dev Perplexity 2.0266019543030334\n",
      "Epoch:  998\n",
      "| Iter 105685 | Avg Train Loss 0.037086846828460696 | Dev Perplexity 2.0266019543030334\n",
      "| Iter 105690 | Avg Train Loss 0.03415986478328705 | Dev Perplexity 2.0266019543030334\n",
      "| Iter 105695 | Avg Train Loss 0.03361881673336029 | Dev Perplexity 2.0266019543030334\n",
      "| Iter 105700 | Avg Train Loss 0.0365998375415802 | Dev Perplexity 2.027142773403148\n",
      "| Iter 105705 | Avg Train Loss 0.0340661358833313 | Dev Perplexity 2.027142773403148\n",
      "| Iter 105710 | Avg Train Loss 0.03531085729598999 | Dev Perplexity 2.027142773403148\n",
      "| Iter 105715 | Avg Train Loss 0.035422114729881285 | Dev Perplexity 2.027142773403148\n",
      "| Iter 105720 | Avg Train Loss 0.03611656427383423 | Dev Perplexity 2.0168572411189762\n",
      "| Iter 105725 | Avg Train Loss 0.034050163626670835 | Dev Perplexity 2.0168572411189762\n",
      "| Iter 105730 | Avg Train Loss 0.034088413119316104 | Dev Perplexity 2.0168572411189762\n",
      "| Iter 105735 | Avg Train Loss 0.0333747136592865 | Dev Perplexity 2.0168572411189762\n",
      "| Iter 105740 | Avg Train Loss 0.03399131715297699 | Dev Perplexity 2.01339422300364\n",
      "| Iter 105745 | Avg Train Loss 0.03473074972629547 | Dev Perplexity 2.01339422300364\n",
      "| Iter 105750 | Avg Train Loss 0.03682653784751892 | Dev Perplexity 2.01339422300364\n",
      "| Iter 105755 | Avg Train Loss 0.03474666655063629 | Dev Perplexity 2.01339422300364\n",
      "| Iter 105760 | Avg Train Loss 0.03568168163299561 | Dev Perplexity 2.016348112982197\n",
      "| Iter 105765 | Avg Train Loss 0.03617402911186218 | Dev Perplexity 2.016348112982197\n",
      "| Iter 105770 | Avg Train Loss 0.03473219096660614 | Dev Perplexity 2.016348112982197\n",
      "| Iter 105775 | Avg Train Loss 0.0341864150762558 | Dev Perplexity 2.016348112982197\n",
      "| Iter 105780 | Avg Train Loss 0.03436080634593964 | Dev Perplexity 2.015247444846815\n",
      "| Iter 105785 | Avg Train Loss 0.033416720628738406 | Dev Perplexity 2.015247444846815\n",
      "Epoch:  999\n",
      "| Iter 105790 | Avg Train Loss 0.034796364307403564 | Dev Perplexity 2.015247444846815\n",
      "| Iter 105795 | Avg Train Loss 0.03369578301906586 | Dev Perplexity 2.015247444846815\n",
      "| Iter 105800 | Avg Train Loss 0.03345287561416626 | Dev Perplexity 2.026089227610735\n",
      "| Iter 105805 | Avg Train Loss 0.03575089752674103 | Dev Perplexity 2.026089227610735\n",
      "| Iter 105810 | Avg Train Loss 0.03553200244903564 | Dev Perplexity 2.026089227610735\n",
      "| Iter 105815 | Avg Train Loss 0.03577950298786163 | Dev Perplexity 2.026089227610735\n",
      "| Iter 105820 | Avg Train Loss 0.03498372852802276 | Dev Perplexity 2.020029606464854\n",
      "| Iter 105825 | Avg Train Loss 0.03428865790367126 | Dev Perplexity 2.020029606464854\n",
      "| Iter 105830 | Avg Train Loss 0.034950242042541504 | Dev Perplexity 2.020029606464854\n",
      "| Iter 105835 | Avg Train Loss 0.035170727968215944 | Dev Perplexity 2.020029606464854\n",
      "| Iter 105840 | Avg Train Loss 0.03404451727867126 | Dev Perplexity 2.021415147623227\n",
      "| Iter 105845 | Avg Train Loss 0.03494817316532135 | Dev Perplexity 2.021415147623227\n",
      "| Iter 105850 | Avg Train Loss 0.03663022816181183 | Dev Perplexity 2.021415147623227\n",
      "| Iter 105855 | Avg Train Loss 0.035334819555282594 | Dev Perplexity 2.021415147623227\n",
      "| Iter 105860 | Avg Train Loss 0.03624063909053803 | Dev Perplexity 2.0183100186299\n",
      "| Iter 105865 | Avg Train Loss 0.03470969319343567 | Dev Perplexity 2.0183100186299\n",
      "| Iter 105870 | Avg Train Loss 0.0347816264629364 | Dev Perplexity 2.0183100186299\n",
      "| Iter 105875 | Avg Train Loss 0.03361375153064728 | Dev Perplexity 2.0183100186299\n",
      "| Iter 105880 | Avg Train Loss 0.032636205554008486 | Dev Perplexity 2.015311897822334\n",
      "| Iter 105885 | Avg Train Loss 0.03409251511096954 | Dev Perplexity 2.015311897822334\n",
      "| Iter 105890 | Avg Train Loss 0.03431189000606537 | Dev Perplexity 2.015311897822334\n",
      "Epoch:  1000\n",
      "| Iter 105895 | Avg Train Loss 0.033960360884666446 | Dev Perplexity 2.015311897822334\n",
      "| Iter 105900 | Avg Train Loss 0.03513773858547211 | Dev Perplexity 2.0208742218632394\n",
      "| Iter 105905 | Avg Train Loss 0.03462380409240723 | Dev Perplexity 2.0208742218632394\n",
      "| Iter 105910 | Avg Train Loss 0.032264690995216366 | Dev Perplexity 2.0208742218632394\n",
      "| Iter 105915 | Avg Train Loss 0.032975354194641114 | Dev Perplexity 2.0208742218632394\n",
      "| Iter 105920 | Avg Train Loss 0.03556068599224091 | Dev Perplexity 2.0338169553488354\n",
      "| Iter 105925 | Avg Train Loss 0.03343285977840424 | Dev Perplexity 2.0338169553488354\n",
      "| Iter 105930 | Avg Train Loss 0.03220166563987732 | Dev Perplexity 2.0338169553488354\n",
      "| Iter 105935 | Avg Train Loss 0.03382282555103302 | Dev Perplexity 2.0338169553488354\n",
      "| Iter 105940 | Avg Train Loss 0.033974280953407286 | Dev Perplexity 2.0472982193674083\n",
      "| Iter 105945 | Avg Train Loss 0.0342487770318985 | Dev Perplexity 2.0472982193674083\n",
      "| Iter 105950 | Avg Train Loss 0.038296248316764835 | Dev Perplexity 2.0472982193674083\n",
      "| Iter 105955 | Avg Train Loss 0.03434638857841492 | Dev Perplexity 2.0472982193674083\n",
      "| Iter 105960 | Avg Train Loss 0.03641498327255249 | Dev Perplexity 2.0429696748472477\n",
      "| Iter 105965 | Avg Train Loss 0.035790209770202634 | Dev Perplexity 2.0429696748472477\n",
      "| Iter 105970 | Avg Train Loss 0.0380764240026474 | Dev Perplexity 2.0429696748472477\n",
      "| Iter 105975 | Avg Train Loss 0.03612615406513214 | Dev Perplexity 2.0429696748472477\n",
      "| Iter 105980 | Avg Train Loss 0.03281234741210937 | Dev Perplexity 2.0199575206668627\n",
      "| Iter 105985 | Avg Train Loss 0.03495411038398743 | Dev Perplexity 2.0199575206668627\n",
      "| Iter 105990 | Avg Train Loss 0.03360628426074982 | Dev Perplexity 2.0199575206668627\n",
      "| Iter 105995 | Avg Train Loss 0.03478721976280212 | Dev Perplexity 2.0199575206668627\n",
      "| Iter 106000 | Avg Train Loss 0.03345108330249787 | Dev Perplexity 2.0212001436637284\n"
     ]
    }
   ],
   "source": [
    "run(model, train_df, device, train_data_loader, val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}